{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install --upgrade --quiet unstructured\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install -q streamlit\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install --upgrade --quiet  langchain-google-genai\n",
        "# !pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "tl_0E9sv0NWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "OwN7V8WG0NC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYKTvxG80BP0"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
        "import os\n",
        "\n",
        "st.title(\"LLM Chatbot News Insights\")\n",
        "st.sidebar.title(\"News Articles URLs\")\n",
        "\n",
        "# st.set_page_config(page_title = 'News Research Tool')\n",
        "st.markdown(\"\"\"\n",
        "## Get instant insights from online news\n",
        "## Follow the instructions\n",
        "\n",
        "1. Get your Google API key from here - https://ai.google.dev/gemini-api/docs/api-key\n",
        "2. Copy Paste the online news links.\n",
        "3. Ask your question.\n",
        "4. Press \"Process URLs\" button.\n",
        "\"\"\")\n",
        "\n",
        "api_key = st.text_input('Enter your Google API key: ', type='password', key='api_key_input')\n",
        "\n",
        "def get_text_from_urls(urls):\n",
        "    try:\n",
        "        loader = UnstructuredURLLoader(urls=urls)\n",
        "        data = loader.load()\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading URLs: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_text_chunks(data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        separators=['\\n\\n', \"\\n\", '.', ' ']\n",
        "    )\n",
        "    docs = text_splitter.split_documents(data)\n",
        "    return docs\n",
        "\n",
        "def get_vector_store(docs, api_key):\n",
        "    try:\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
        "                                                  google_api_key=api_key)\n",
        "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "        os.makedirs('faiss_index', exist_ok=True)\n",
        "\n",
        "        vectorstore.save_local('faiss_index')\n",
        "        # st.success(\"Vector store saved successfully.\")\n",
        "        # vectorstore.save_local('faiss_index')\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error creating vector store: {e}\")\n",
        "\n",
        "def get_conversational_chain(api_key):\n",
        "    prompt_template = \"\"\"\n",
        "        Answer the question as detailed as possible from the provided context, make sure to provide all the details,\n",
        "        if the answer is not in the provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
        "        Context:\\n {context}?\\n\n",
        "        Question: \\n{question}\\n\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.3, google_api_key=api_key)\n",
        "    prompt = PromptTemplate(\n",
        "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "    return chain\n",
        "\n",
        "def user_input(user_question, api_key):\n",
        "    try:\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
        "        if not os.path.exists('faiss_index'):\n",
        "            st.error(\"Vector store index file not found. Please process URLs first.\")\n",
        "            return\n",
        "\n",
        "        new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "        docs = new_db.similarity_search(user_question)\n",
        "        chain = get_conversational_chain(api_key)\n",
        "        response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
        "        st.write(\"Reply: \", response['output_text'])\n",
        "        return response['output_text']\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing user input: {e}\")\n",
        "\n",
        "def main():\n",
        "    st.header(\"LLM Chatbot News Insights\")\n",
        "    user_question = st.text_input(\"Ask your question: \", key=\"user_question\")\n",
        "    if user_question and api_key:\n",
        "        user_input(user_question, api_key)\n",
        "\n",
        "    urls = []\n",
        "    for i in range(1):\n",
        "        url = st.sidebar.text_input(f\"URL {i+1}\")\n",
        "        if url:\n",
        "            urls.append(url)\n",
        "\n",
        "    main_placeholder = st.empty()\n",
        "    process_url_clicked = st.sidebar.button(\"Process URLs\")\n",
        "    if process_url_clicked and api_key and urls:\n",
        "        with st.spinner(\"Processing URLs...\"):\n",
        "            data = get_text_from_urls(urls)\n",
        "            if data:\n",
        "                docs = get_text_chunks(data)\n",
        "                if docs:\n",
        "                    get_vector_store(docs, api_key)\n",
        "                    st.success(\"Done\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "BvKKsNbz0Q9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "bF9MJ3Dj0Rvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}